\documentclass[12pt]{article}

\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{cite}

\hypersetup{
    bookmarks=true,         % show bookmarks bar?
      colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\input{../Comments}
\input{../Common}

% For easy change of table widths
\newcommand{\colZwidth}{1.0\textwidth}
\newcommand{\colAwidth}{0.13\textwidth}
\newcommand{\colBwidth}{0.82\textwidth}
\newcommand{\colCwidth}{0.1\textwidth}
\newcommand{\colDwidth}{0.05\textwidth}
\newcommand{\colEwidth}{0.8\textwidth}
\newcommand{\colFwidth}{0.17\textwidth}
\newcommand{\colGwidth}{0.5\textwidth}
\newcommand{\colHwidth}{0.28\textwidth}

% Numbering 
\usepackage{amsthm}
\usepackage{xassoccnt}
%\newtheorem{req}{Requirement}[section]        
\newtheorem{req}{Requirement}        
\theoremstyle{definition}        
\newtheorem{constraint}{Constraint}
\newtheorem{goal}{Goal}
\DeclareCoupledCountersGroup{theorems}
\DeclareCoupledCounters[name=theorems]{req,constraint,goal}
\setcounter{goal}{0}

\usepackage{fullpage}

\setlength{\parindent}{0pt} % Set no indent to entire document.

\begin{document}

\title{Software Requirements Specification for \progname} 
\author{\authname}
\date{\today}
	
\maketitle

~\newpage

\pagenumbering{roman}

\tableofcontents

~\newpage

\section*{Revision History}


\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2025-10-06 & 1.0 & Initial Write-up\\
\bottomrule
\end{tabularx}


~\newpage

\section{Goal}

\subsection{G.1 Context and overall objective}

With around 4 million Canadians affected by hearing loss \cite{Healthing2025}, 
there is a significant need for assistive technologies that can improve 
situational awareness and safety.
Many safety cues and general sound alerts such as the sound of a car 
approaching, a kettle whistling,
or a phone ringing may be missed, leading to increased risk of injury and
miscommunication.

Many existing solutions focus on speech transcription, but lack the ability to
provide directional information about sound sources or classify non-speech
sounds. This project aims to address this gap by developing an assistive device
that provides real-time visual indications of sound source locations and 
classifications.

The objective of this project is to develop an assistive device that aids
individuals who are deaf or hard of hearing by providing real-time visual
indications of sound source locations and classifications (ex. 'car on your 
left').

Some of the high-level goals of the project are:

\begin{goal}\label{goal:audio_capture}
Capture real-time audio data from a 
\hyperref[def:microphone_array]{microphone array}
with synchronized sampling to enable accurate situational analysis
of sound sources.
\end{goal}

\begin{goal}\label{goal:audio_direction_analysis}
Analyze captured audio to determine the direction of arrival (DoA) of
sound sources with minimal error and with minimal latency nearing real-time.
\end{goal}

\begin{goal}\label{goal:audio_identification_analysis}
Analyze captured audio to classify the sound sources with their English
label (ex. 'car', 'phone', 'kettle', 'alarm', 'speech').
\end{goal}

\begin{goal}\label{goal:visual_display}
Display audio classification and transcription on smart glasses in real-time
without obstructing the user's field of view.
\end{goal}

\begin{goal}\label{goal:user_friendly_interaction}
Provide a user-friendly interaction with the smart glasses,
allowing the user to easily set up, use, and understand visual indicators.
\end{goal}

\begin{goal}\label{goal:user_comfort}
Ensure that the system is comfortable to wear for extended periods of time,
with minimal discomfort or fatigue.
\end{goal}
    
\subsection{G.2 Current situation}

Currently, individuals who are deaf or hard of hearing face significant
challenges in maintaining situational awareness due to missed audio cues.
Existing assistive technologies address some aspects of this problem, but
leave critical gaps:

\begin{itemize}
\item \textbf{Smart glasses with transcription capabilities:} Some devices
can listen to live human audio and transcribe it to text (multilingual) in
real-time, displaying the transcript on a smartphone display.
However, these solutions focus solely on speech transcription and do not
provide directional information about sound sources or classify non-speech
sounds.

\item \textbf{Hearing aids:} Traditional hearing aids amplify ambient sounds
to improve awareness of audio sources at various volumes \cite{NIDCD2022}. 
While this helps individuals with partial hearing loss, it does not assist 
those who are profoundly deaf, nor does it provide visual cues about sound 
direction or classification.

\item \textbf{Notification systems:} Some home automation systems can send
visual alerts (e.g., flashing lights) when specific sounds are detected,
such as doorbells or smoke alarms. However, these systems are limited to
fixed locations and predetermined sound types, lacking portability and
real-time directional awareness.
\end{itemize}

The current solutions fail to address the critical need for real-time,
portable, directional awareness of environmental sounds, leaving individuals
vulnerable to missing important safety cues such as approaching vehicles,
warning beeps from machinery, or emergency alerts.

\subsection{G.3 Expected benefits}

The proposed system will deliver significant improvements to the daily lives
and safety of individuals who are deaf or hard of hearing:

\begin{itemize}
\item \textbf{Real-time spatial awareness:} Enable identification of sound
source locations on a 2D plane in real-time, allowing users to quickly
orient themselves toward important sounds such as someone calling their name,
an approaching vehicle, or an emergency alarm.

\item \textbf{Enhanced safety:} Reduce the risk of injury by alerting users
to critical safety cues that are typically communicated through sound, such
as warning beeps from forklifts, tea kettles whistling, car engines
or emergency sirens (from emergency vehicles) approaching from behind.

\item \textbf{Improved situational awareness:} Provide continuous awareness
of the acoustic environment without requiring the user to constantly scan
their surroundings, reducing cognitive load and enabling more natural
interactions with their environment.

\item \textbf{Sound classification:} Differentiate between various types of
sounds (e.g., speech, alarms, vehicles, household appliances) to help users
prioritize their attention and responses appropriately.

\item \textbf{Reduced frustration and miscommunication:} Minimize instances
of missed phone calls, doorbell rings, or verbal attempts to gain the user's
attention, leading to smoother social interactions and reduced social
isolation.

\item \textbf{Portable and wearable solution:} Unlike fixed home automation
systems, the smart glasses form factor provides continuous protection and
awareness regardless of location, whether at home, work, or in public spaces.

\item \textbf{Independence and confidence:} Empower users to navigate their
environment more independently without relying on others to alert them to
important sounds, fostering greater autonomy in daily activities.
\end{itemize}



\subsection{G.4 Functionality overview}

The system will provide the following principal functions:

\begin{itemize}
\item \textbf{Real-time audio capture:} Continuously capture audio signals
from a synchronized microphone array mounted on smart glasses, ensuring
precise temporal alignment for accurate spatial analysis.

\item \textbf{Direction of arrival (DoA) estimation:} Process captured audio
to determine the angular direction of sound sources on a 2D plane relative
to the user's position, with a target accuracy of ±45° for single sound
sources.

\item \textbf{Sound source classification:} Analyze audio characteristics to
classify detected sounds into meaningful categories (e.g., speech, vehicle
sounds, alarms, household appliances) using audio fingerprinting techniques,
with a target accuracy of at least 90\%.

\item \textbf{Visual feedback generation:} Generate intuitive visual
representations of detected sound sources, including their direction and
classification, displayed on the smart glasses interface with minimal
latency ($\leq$1 second).

\item \textbf{Multi-source handling:} Detect and track multiple simultaneous
sound sources when feasible, prioritizing the most relevant or critical
sounds based on classification and proximity.

\item \textbf{Real-time processing:} Execute all signal processing,
direction estimation and classification algorithms real-time,
with consistent performance and low latency.

\item \textbf{Noise cancellation or audio filtering:} The system will
modify or filter the actual sounds in the environment based on direction
of arrival in order to improve directional hearing. This would help improve
the quality of the transcriptions provided by the system.
\end{itemize}

\subsection{G.5 High-level usage scenarios}

The following scenarios illustrate fundamental usage paths through the system:

\subsubsection{Scenario 1: Pedestrian crossing detection}
A user is walking in an urban environment and approaches a street
intersection. As they prepare to cross, a car approaches from their left
side. The system detects the engine sound, estimates its direction (e.g.,
90° to the left), classifies it as a vehicle, and displays a visual
indicator on the smart glasses showing the direction and classification.
The user recognizes the alert and waits for the vehicle to pass before
crossing safely.

\subsubsection{Scenario 2: Kitchen safety alert}
A user is cooking in their kitchen when a tea kettle on the stove begins
to whistle. The system captures the high-pitched sound through the
microphone array, determines that it is coming from behind and to the
right (e.g., 135°), classifies it as a kettle or alarm sound, and displays
a directional indicator. The user turns toward the alert and removes the
kettle from heat, preventing a potential hazard.

\subsubsection{Scenario 3: Social interaction}
A user is in a crowded room when someone calls their name from across the
space. The system detects the speech sound, estimates the direction (e.g.,
30° to the right), classifies it as speech or a human voice, and displays
the information on the glasses. The user turns in the indicated direction
to make eye contact and engage in conversation, reducing social friction
and missed interactions.

\subsubsection{Scenario 4: Workplace awareness}
A user is working in an industrial setting when a forklift begins reversing
nearby, emitting a warning beep. The system detects the beeping pattern,
determines its direction (e.g., directly behind at 180°), classifies it as
a warning signal, and alerts the user with a prominent visual indicator.
The user steps aside to maintain a safe distance from the moving equipment.

\subsection{G.6 Limitations and exclusions}

The following aspects are explicitly outside the scope of this project:

\begin{itemize}
\item \textbf{Autonomous danger assessment:} The system will not independently
evaluate whether a detected sound represents an immediate danger or
automatically alert the user of hazardous situations. It will present
directional and classification information, leaving interpretation and
response decisions to the user.

\item \textbf{Augmented reality overlay:} The system will not provide
full augmented reality capabilities with spatial overlays showing sound
locations directly mapped onto the user's field of view. Visual feedback
will be presented through a simpler display interface on the smart glasses.

\item \textbf{User response monitoring:} The system will not track whether
the user has noticed, acknowledged, or responded to presented alerts. There
is no feedback loop to ensure user reaction or to escalate notifications.

\item \textbf{Multilingual speech transcription:} Audio transcription
functionality, if implemented, will be limited to English only. Support
for other languages is not included in the current scope.

\item \textbf{3D spatial localization:} Direction estimation will be
constrained to a 2D horizontal plane around the user. Elevation angle
determination (above or below the user's head level) is excluded from
the core functionality.

\item \textbf{Sound source distance estimation:} While direction will be
provided, the system will not attempt to estimate the absolute distance
to sound sources.

\item \textbf{Continuous recording or data storage:} The system will not
record or store audio data beyond what is necessary for real-time processing.
No historical logs of detected sounds will be maintained.

\item \textbf{Network connectivity:} All processing will
occur locally on the embedded hardware. The system will not require internet
connectivity or cloud-based services for core functionality.
\end{itemize}

\subsection{G.7 Stakeholders and requirements sources}

\subsubsection{Primary stakeholders}

\begin{itemize}
\item \textbf{Individuals who are deaf or hard of hearing:} The primary
end-users of the system, who will directly benefit from improved
situational awareness and safety. This group is quite large in population,
with approximately 4 million people who experience hearing loss 
in Canada alone (1 in 10).
\end{itemize}

\subsubsection{Secondary stakeholders}

\begin{itemize}
\item \textbf{Family members and caregivers:} Individuals who support people
with hearing loss and will benefit from improved communication and reduced
safety concerns.

\item \textbf{Employers and workplace safety officers:} Organizations that
employ individuals with hearing loss and are responsible for maintaining
safe working environments.

\item \textbf{Accessibility advocates and organizations:} Groups focused on
improving quality of life and independence for individuals with disabilities.

\item \textbf{Healthcare providers and audiologists:} Professionals who may
recommend or integrate such assistive technologies into patient care plans.

\item \textbf{Future developers and researchers:} The broader engineering
and scientific community who may build upon this work or apply similar
techniques to related problems.
\end{itemize}

\subsubsection{Requirements sources}

\begin{itemize}
\item \textbf{Academic literature:} Research on hearing loss impact,
assistive technologies, direction of arrival algorithms, and audio
classification techniques.

\item \textbf{Domain experts:}\label{itm:domain-experts} Consultation with domain experts, such as 
Dr. Mohrenschildt, for technical feasibility and requirements validation.

\item \textbf{Existing assistive technologies:} Analysis of current solutions
such as hearing aids, transcription glasses, and home alert systems to
identify gaps and opportunities.

\item \textbf{Hardware and software documentation:} Technical specifications
for microcontroller, source code libraries, smart glasses hardware,
and microphone array components.

\item \textbf{Standards and best practices:} IEEE standards for embedded
systems, accessibility guidelines, and real-time system design principles.

\item \textbf{Proof of concept testing:} Empirical results from prototyping
and laboratory testing to validate technical approaches and refine
requirements.
\end{itemize}


\section{Environment}

\subsection{E.1 Glossary}

\begin{itemize}
    \item \textbf{Microphone Array:}\label{def:microphone_array} A collection of 
    microphones that are synchronized to capture audio from the 
    environment to create a single multi-channel audio signal.

    \item \textbf{Microcontroller:}\label{def:microcontroller} Compact embedded
    hardware system with a CPU, memory, input/output peripherals, designed to
    process real-time data directly on the device.

    \item \textbf{Microphone:}\label{def:microphone} Input sensor to a device
    that converts audio soundwaves to digital bytestream.

    \item \textbf{Normal Operating Condition:}
    \label{def:normal_operation_condition} The state in which the system,
    including hardware and softwre, operate within expected environmental and
    usage parameters. This includes audio input sampled at the correct sample
    rate, input audio signals are within the human audible range
    (20 Hz – 20 kHz) \cite{Neuroscience2001}, and audio input levels below
    90 dB.

    \item \textbf{Spectral Leakage:} \label{def:spectral_leakage} 
    The spreading of a signal's energy across multiple frequency bins in a
    frequency spectrum due to finite time windowing. Essentially, noise that is
    caused by sharp signal cutting when processing audio signals as windows.

\end{itemize}

\subsection{E.2 Components}
\wss{List of elements of the environment that may affect or be affected by the system and project. Includes other systems to which the system must be interfaced.}

\subsection{E.3 Constraints}
\wss{Obligations and limits imposed on the project and system by the environment.}

\subsection{E.4 Assumptions}
\wss{Properties of the environment that may be assumed, with the goal of facilitating the project and simplifying the system.}

\subsection{E.5 Effects}
\wss{Elements and properties of the environment that the system will affect.}

\subsection{E.6 Invariants}
\wss{Properties of the environment that the systemâ€™s operation must preserve.}

\section{System}

\subsection{S.1 Components}

\begin{figure}[H]     
    \centering 
    \includegraphics[width=0.9\textwidth]{diagrams/s1_component_diagram.png}
    \caption{Software component diagram.}
    \label{fig:software_component_diagram}
\end{figure}

\subsubsection{Software Components}

\begin{itemize}
  \item \textbf{Embedded Firmware:} \label{comp:embedded_firmware}
  Main component responsible for managing all embedded software on the
  Microcontroller.

  \item \textbf{Driver Layer:} \label{comp:driver_layer}
  Component responsible for providing drivers on the microcontroller. It
  provides interfaces for high level applications to interact with
  hardware components external to the microcontroller

  \item \textbf{Audio Filtering:} \label{comp:audio_filtering}
  Component responsible for processing raw audio signals in real-time sent
  by the microphones.

  \item \textbf{Calibration Component:} \label{comp:calibration}
  Component for calibrating hardware including microphones and output display.
  
  \item \textbf{Audio360 Engine:} \label{comp:audio360Engine}
  Main component responsible for running Audio360 features. It serves as the
  primary interface and controller for the Audio360 system. The component is
  designed to be hardware agnostic, allowing seamless integration with any
  firmware running on other hardware.
  
  \item \textbf{Frequency Analysis:} \label{comp:frequency_analysis}
  Component responsible for analyzing properties of processed frequency signals.
  Sub components of this includes audio classification and direction of arrival
  estimation.
  
  \item \textbf{Visualization Controller:} \label{comp:viz_controller}
  Component reposonsible for creating and sending visualization output to the
  \hyperref[comp:display]{output display}.
\end{itemize}

\subsubsection{Hardware Components}

\begin{itemize}
  \item \textbf{Microphone:}\label{comp:microphone}
  Component responsible for collecting audio signals from the environment.

  \item \textbf{Output display:}\label{comp:display} Component responsible for
  displaying visuals to the user.

  \item \textbf{Microcontroller:}
  \label{comp:microcontroller} Component responsible for executing the main
  software and interfacing with input and output peripheral devices.
\end{itemize}

\subsection{S.2 Functionality}

\subsubsection{\hyperref[comp:embedded_firmware]{Embedded Firmware}}
Functional Requirements
\begin{itemize}
  \item \label{FR1_1}\textbf{FR1.1:} The firmware shall schedule tasks
  based on its priority.
  
  \item \label{FR1_2}\textbf{FR1.2:} The firmware shall process and
  synchronize audio signals from all microphones connected to the
  microcontroller.

  \item \label{FR1_3}\textbf{FR1.3:} The firmware shall handle memory errors to
  prevent the micrcontroller from crashing.

  \item \label{FR1_4}\textbf{FR1.4:} The system shall perform continuous
  diagnostics on all hardware components to monitor hardware errors in
  real-time. This ensures the system can react to failures as soon as they 
  occur.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR1_1}\textbf{NFR1.1:} The firmware shall process audio
  signals in real-time with monotonic frame sequences. Earliest frames shall
  have higher priority.
  
  \item \label{NFR1_2}\textbf{NFR1.2:} The firmware shall operate
  faster than the connected microphone sample rate 44100 Hz, refering to
  \hyperref[NFR7_1]{NFR7.1}.
\end{itemize}

\subsubsection{\hyperref[comp:driver_layer]{Driver Layer}}
Functional Requirements
\begin{itemize}
  \item \label{FR2_1}\textbf{FR2.1:} The driver layer shall provide an
  interface for higher level software to interact with the hardware on the
  microcontroller.

  \item \label{FR2_2}\textbf{FR2.2:} The driver layer shall process 
  hardware interface requests based on system permissions of the requester.

  \item \label{FR2_3}\textbf{FR2.3:} The driver layer shall return error codes 
  upon return to higher-level software to support error handling.

  \item \label{FR2_4}\textbf{FR2.4:} The driver shall maintain data integrity
  on memory slots that is actively being used.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR2_1}\textbf{NFR2.1:} The driver shall immediately propagate
  any errors to the firmware layer for proper error handling.
\end{itemize}

\subsubsection{\hyperref[comp:audio_filtering]{Audio Filtering}} 
Functional Requirements
\begin{itemize}
  \item \label{FR3_1}\textbf{FR3.1:} The audio filtering component shall
  convert digital audio waveform to frequency domain.

  \item \label{FR3_2}\textbf{FR3.2:} The audio filtering component shall
  normalize the amplitude of incoming and outgoing signals.

  \item \label{FR3_3}\textbf{FR3.3:} The audio filtering component shall
  filter incoming audio signals to reduce frequency
  \hyperref[def:spectral_leakage]{spectral leakage}.

  \item \label{FR3_4}\textbf{FR3.4:} The audio filtering component shall
  use available hardware acceleration method for efficient computations.

  \item \label{FR3_5}\textbf{FR3.5:} The audio filtering component shall
  detect and flag audio anomalies such as clipping, lost signal and silence.
  This enables identification of microphone or signal path faults.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR3_1}\textbf{NFR3.1:} The audio filtering component shall 
  represent audio signals in the frequency domain with less than 10\% error from
  the true value.
  \item \label{NFR3_2}\textbf{NFR3.2:} The audio filtering component shall be
  scalable to handle different input signal sizes without missing timing
  constraints defined in \hyperref[NFR1_2]{NFR1.2} for input signal size up to
  4096 frames.

  \item \label{NFR3_3}\textbf{NFR3.3:} The audio filtering component shall have
  an audio processing success rate end to end of atleast 90\% over 60 seconds.
\end{itemize}

\subsubsection{\hyperref[comp:audio360Engine]{Audio360 Engine}} 
Functional Requirements
\begin{itemize}
  \item \label{FR4_1}\textbf{FR4.1:} The Audio360 Engine shall retrieve data,
  such as frequency domain and errors, from the microcontroller via the driver
  layer for further processing.

  \item \label{FR4_2}\textbf{FR4.2:} The Audio360 Engine shall notify dependent
  components when new frequency data is available. The latest data shall always
  be used.

  \item \label{FR4_3}\textbf{FR4.3:} The Audio360 Engine shall control the flow
  of processed audio data from audio frequency analysis to visualization.

  \item \label{FR4_4}\textbf{FR4.4:} The Audio360 Engine shall disable audio
  classification and directional analysis features until microphone faults
  addressed in \hyperref[FR3_5]{FR3.5} are resolved. This prevents the
  generation of unreliable or unsafe outputs.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR4_1}\textbf{NFR4.1:} The Audio360 Engine shall poll data
  without conflicting with ongoing microcontroller memory writes, ensuring no
  data loss.

  \item \label{NFR4_2}\textbf{NFR4.2:} The Audio360 Engine shall retrieve
  frequency data within the timing constraints of the embedded firmware defined
  in \hyperref[NFR1_2]{NFR1.2}.

  \item \label{NFR4_3}\textbf{NFR4.3:} The Audio360 Engine shall permanently
  discard microphone audio data immediately after completion of audio analysis.
\end{itemize}

\subsubsection{\hyperref[comp:frequency_analysis]{Frequency Analysis}} 
Functional Requirements
\begin{itemize}
  \item \label{FR5_1}\textbf{FR5.1:} The frequency analysis component shall
  classify sound sources based on features extracted from the frequency domain
  representation of the audio input signal.

   \item \label{FR5_2}\textbf{FR5.2:} The frequency analysis component shall
   estimate the direction of arrival of the audio source using frequency domain
   representation of the audio input signal. 
   
   \item \label{FR5_3}\textbf{FR5.3:} The frequency analysis component shall 
   represent the direction of arrival as an angle ($\theta$) in radians.
   $\theta$ is measured relative to the forward axis of the glasses frame
   to the sound source, Figure \ref{fig:coordinate_system}.

   \item \label{FR5_4}\textbf{FR5.4:} The frequency analysis component shall
   notify users when a sound classification result has low confidence or is
   unrecognized to prevent misleading contextual feedback.
\end{itemize}

\begin{figure}[H]     
    \centering 
    \includegraphics[width=0.75\textwidth]{diagrams/glasses_coordinate.png}
    \caption{Coordinate System.}
    \label{fig:coordinate_system}
\end{figure}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR5_1}\textbf{NFR5.1:} The frequency analysis shall classify
  atleast 3 distinct sound sources relevant to people who are hard of hearing.

  \item \label{NFR5_2}\textbf{NFR5.2:} The frequency analysis component shall
  achieve a minimum classification accuracy of 90\% under
  \hyperref[def:normal_operation_condition]{normal operating conditions}.

  \item \label{NFR5_3}\textbf{NFR5.3:} The component shall estimate the
  direction of arrival of an audio source with a maximum error of 0.78 rad.
  This ensures reliable directional awareness for the user.
\end{itemize}

\subsubsection{\hyperref[comp:viz_controller]{Visualization Controller}}
Functional Requirements
\begin{itemize}
  \item \label{FR6_1}\textbf{FR6.1:} The visualization controller component
  shall notify users of the direction of an audio source whenever it is
  detected by the system.

  \item \label{FR6_2}\textbf{FR6.2:} The visualization controller component
  shall alert users when the core safety features such as direction
  determination or classification fail. This ensures that users are aware of
  degraded safety functions.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR6_1}\textbf{NFR6.1:} The visualization controller component
  shall prioritize to display the most safety-critical information at the
  time because display space is limited.
  \item \label{NFR6_2}\textbf{NFR6.2:} The visualization controller component
  shall present information in a non-intrusive manner, minimizing visual
  obstruction so users can safely perform external activities.
\end{itemize}

\subsubsection{\hyperref[comp:microphone]{Microphone}}
Functional Requirements
\begin{itemize}
  \item \label{FR7_1}\textbf{FR7.1:} The microphone shall collect soundwaves
  from its environment and translate to digital representation.

  \item \label{FR7_2}\textbf{FR7.2:} The microphone shall collect soundwaves
  with the human audiable range (20 Hz - 20 kHz) \cite{Neuroscience2001}.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR7_1}\textbf{NFR7.1:} The microphone shall collect soundwaves
  at 44100 Hz. According to Nyquist's theorem, audio sampling should be greater
  than two times the highest frequency to avoid aliasing \cite{DataForth2025}.
  The maximum frequecy audible to humans is approximately 20 kHz.
  \cite{Neuroscience2001}.
\end{itemize}

\subsubsection{\hyperref[comp:display]{Output Display}}
Functional Requirements
\begin{itemize}
  \item \label{FR8_1}\textbf{FR8.1:} The output display shall notify users of 
  the classified sound source.

  \item \label{FR8_2}\textbf{FR8.2:} The output display shall notify users of 
  the direction of the sound source.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR8_1}\textbf{NFR8.1:} The output display shall operate at a
  minimum of 30 display updates per second to ensure low latency and provide
  information to the user without perceptible delay.
\end{itemize}

\subsubsection{\hyperref[comp:microcontroller]{Microcontroller}}
Functional Requirements
\begin{itemize}
  \item \label{FR9_1}\textbf{FR9.1:} The microcontroller shall run the main
  \progname ~software in a closed environment. No external devices
  other than the microphones and output display shall be able to connect to it.
  This ensures protection against unauthorized access.
\end{itemize}

Non-Functional Requirements
\begin{itemize}
  \item \label{NFR9_1}\textbf{NFR9.1:} The microcontroller shall have a clock 
  speed of atleast 100 MHz to support audio sampling at 44100 Hz, derived
  from \hyperref[NFR7_1]{NFR7.1}.

  \item \label{FR9_2}\textbf{NFR9.2:} The microcontroller shall provide atleast
  four microphone inputs to support directional analysis of audio.
\end{itemize}


\subsection{S.3 Interfaces}
Audio360 is designed as a closed embedded system due to its safety-critical
application domain, refering to functional requirement \hyperref[FR9_1]{FR9.1}.
External access is intentionally restricted to ensure reliability, security,
and user safety.
The only user interface is the integrated visual display. The display provides
real-time information by notifying users of detected sounds and their
corresponding direction.
No external programmatic interfaces (APIs) will be provided. This design
decision ensures that system integrity is maintained and that no external
software can interfere with the safety-critical operation of the device.

\subsection{S.4 Detailed usage scenarios}
\wss{Examples of interaction between the environment (or human users) and the system: use cases, user stories.}

\subsection{S.5 Prioritization}
\wss{Classification of the behaviors, interfaces and scenarios (S.2, S.3 and S.4) by their degree of criticality.}

\subsection{S.6 Verification and acceptance criteria}
\wss{Specification of the conditions under which an implementation will be deemed satisfactory.}

\section{Project}

\subsection{P.1 Roles and personnel}
\wss{Main responsibilities in the project; required project staff and their needed qualifications.}

\subsection{P.2 Imposed technical choices}
\wss{Any a priori choices binding the project to specific tools, hardware, languages or other technical parameters.}

\subsection{P.3 Schedule and milestones}
\wss{List of tasks to be carried out and their scheduling.}

\subsection{P.4 Tasks and deliverables}
\wss{Details of individual tasks listed under P.3 and their expected outcomes.}

\subsection{P.5 Required technology elements}
\wss{External systems, hardware and software, expected to be necessary for building the system.}

\subsection{P.6 Risks and mitigation analysis}
\wss{Potential obstacles to meeting the schedule of P.4, and measures for adapting the plan if they do arise.}

\subsection{P.7 Requirements process and report}
The development of this project requires iterating over various stages of the
requirements process. These stages include:

\begin{enumerate}
    \item \textbf{Elicitation}
    \begin{enumerate}
        \item Conduct stakeholder interviews and surveys to gather users' needs.

        \item Review background documents and research articles describing how
        core features have been implemented in the past.

        \item Consult with the technical supervisor of this application
        (\hyperref[itm:domain-experts]{MVM}) to gain insight into what
        approaches can be used. 
    \end{enumerate}

    \item \textbf{Analysis}
    \begin{enumerate}
        \item Based on information retrieved from the elicitation process,
        derive a list of soft and hard goals for the application.

        \item Using the goals defined previously, derive a list of key
        requirements. This will be influenced by the goals of the stakeholders,
        but also input on potential limitations based on discussion with the
        supervisor.

        \item Group requirements into functional vs.\ non-functional categories.

        \item Prioritize requirements using the MoSCoW framework (Must, Should,
        Could, Won't).

        \item Iteratively evaluate defined requirements based on new constraints
        that arise during implementation. 
    \end{enumerate}

    \item \textbf{Documentation}
    \begin{enumerate}
        \item Write requirements in a structured format that is clear, testable,
        and unambiguous.

        \item Use labels for different requirements and goals for traceability. 
    \end{enumerate}

    \item \textbf{Specification}
    \begin{enumerate}
        \item Using the \texttt{docs/} folder in the main repository for this
        project, update the various reports with the latest information based on
        what was discussed or finalized in other stages of the elicitation
        process. 
    \end{enumerate}

    \item \textbf{Validation}
    \begin{enumerate}
        \item Share draft requirements with stakeholders for confirmation. 
        \item Share requirements with the project supervisor to get expert
        opinion on feasibility of requirements. 
        \item Within the team, ensure the requirements are feasible, measurable,
        and aligned with the project scope. 
    \end{enumerate}
\end{enumerate}


As this project follows the V-model methodology, Figure
\ref{fig:v-model-process}, the team will aim to make the various stages of the
requirements process mentioned above linear. Since although this model allows
the team to re-visit previous parts, it will cost a lot to change in later
stages of the project. This is why this team is heavily prioritizing the initial
stages of the requirements process. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{./images/v_model.png}
    \caption{Illustration of the v-model process.}
    \label{fig:v-model-process}
\end{figure}

\newpage
\bibliographystyle{IEEEtran}
\bibliography{../../refs/References}

\newpage
\section*{Appendix --- Reflection}

\input{../Reflection.tex}

\input{../SRS_Reflection.tex}

\end{document}